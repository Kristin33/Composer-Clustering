{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEC_features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristin33/Composer-Clustering/blob/master/DEC_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46rk7fpWUokm",
        "colab_type": "code",
        "outputId": "088fa5c4-83bc-47dc-9a9b-fe5ceca04df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install keras scikit-learn   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSTAZOA8DNvG",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmPdpfFfDQhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the compositions dataset\n",
        "# the csvs are pre saved in the directory\n",
        "# csv dimension 776\n",
        "def load_comps_csv(files):\n",
        "    print(files)\n",
        "    data_dir = \"drive/My Drive/10701/composer_csv/\"\n",
        "\n",
        "    composer_data = np.empty((0, 776))\n",
        "    composer_label = np.empty((0,))\n",
        "\n",
        "    for idx, filename in enumerate(files):\n",
        "        data = np.genfromtxt(data_dir + filename, delimiter=',', \n",
        "                             usecols=np.arange(1,777), encoding=\"latin1\")\n",
        "        # get rid of the top row with feature names\n",
        "        data = data[1:,:]\n",
        "        # standarize the data\n",
        "        data = stats.zscore(data)\n",
        "        data = np.nan_to_num(data)\n",
        "\n",
        "        composer_data = np.append(composer_data, data, axis=0)\n",
        "        composer_label = np.append(composer_label, np.ones((data.shape[0],)) * idx, axis=0)\n",
        "    # # standarize the data\n",
        "    # data = stats.zscore(composer_data)\n",
        "    # data = np.nan_to_num(composer_data)\n",
        "\n",
        "    x = composer_data\n",
        "    y = composer_label\n",
        "\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "\n",
        "    # 所有的data只需要一个x和y, x是data，y是label\n",
        "    # x就是一个numpy matrix (n, dim), y就是(n, )\n",
        "    return x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgIFmssFHExv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.metrics import v_measure_score, adjusted_rand_score\n",
        "\n",
        "vms = v_measure_score\n",
        "nmi = normalized_mutual_info_score\n",
        "ari = adjusted_rand_score\n",
        "\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate clustering accuracy. Require scikit-learn installed\n",
        "\n",
        "    # Arguments\n",
        "        y: true labels, numpy.array with shape `(n_samples,)`\n",
        "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
        "\n",
        "    # Return\n",
        "        accuracy, in [0,1]\n",
        "    \"\"\"\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "    ind = linear_assignment(w.max() - w)\n",
        "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs889YdfC0Qr",
        "colab_type": "text"
      },
      "source": [
        "# DEC code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XmOxLVzCzT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
        "\n",
        "        Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. ICML 2016.\n",
        "\n",
        "Usage:\n",
        "    use `python DEC.py -h` for help.\n",
        "\n",
        "Author:\n",
        "    Xifeng Guo. 2017.1.30\n",
        "\"\"\"\n",
        "\n",
        "from time import time\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras.layers import Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras import callbacks\n",
        "from keras.initializers import VarianceScaling\n",
        "from sklearn.cluster import KMeans\n",
        "import sklearn\n",
        "from scipy import stats\n",
        "import os\n",
        "\n",
        "\n",
        "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
        "    \"\"\"\n",
        "    Fully connected auto-encoder model, symmetric.\n",
        "    Arguments:\n",
        "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
        "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
        "        act: activation, not applied to Input, Hidden and Output layers\n",
        "    return:\n",
        "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
        "    \"\"\"\n",
        "    n_stacks = len(dims) - 1\n",
        "    # input\n",
        "    x = Input(shape=(dims[0],), name='input')\n",
        "    h = x\n",
        "\n",
        "    # internal layers in encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
        "\n",
        "    # hidden layer\n",
        "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
        "\n",
        "    y = h\n",
        "    # internal layers in decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
        "\n",
        "    # output\n",
        "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
        "\n",
        "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
        "\n",
        "\n",
        "class ClusteringLayer(Layer):\n",
        "    \"\"\"\n",
        "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
        "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
        "\n",
        "    # Example\n",
        "    ```\n",
        "        model.add(ClusteringLayer(n_clusters=10))\n",
        "    ```\n",
        "    # Arguments\n",
        "        n_clusters: number of clusters.\n",
        "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
        "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(n_samples, n_features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
        "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
        "        Arguments:\n",
        "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
        "        Return:\n",
        "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
        "        \"\"\"\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class DEC(object):\n",
        "    def __init__(self,\n",
        "                 dims,\n",
        "                 n_clusters=10,\n",
        "                 alpha=1.0,\n",
        "                 init='glorot_uniform'):\n",
        "\n",
        "        super(DEC, self).__init__()\n",
        "\n",
        "        self.dims = dims\n",
        "        self.input_dim = dims[0]\n",
        "        self.n_stacks = len(self.dims) - 1\n",
        "\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
        "\n",
        "        # prepare DEC model\n",
        "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
        "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
        "\n",
        "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
        "        print('...Pretraining...')\n",
        "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "        # csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
        "        # cb = [csv_logger]\n",
        "        if y is not None:\n",
        "            class PrintACC(callbacks.Callback):\n",
        "                def __init__(self, x, y):\n",
        "                    self.x = x\n",
        "                    self.y = y\n",
        "                    super(PrintACC, self).__init__()\n",
        "\n",
        "                def on_epoch_end(self, epoch, logs=None):\n",
        "                    if int(epochs/10) != 0 and epoch % int(epochs/10) != 0:\n",
        "                        return\n",
        "                    feature_model = Model(self.model.input,\n",
        "                                          self.model.get_layer(\n",
        "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
        "                    features = feature_model.predict(self.x)\n",
        "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
        "                    y_pred = km.fit_predict(features)\n",
        "                    # print()\n",
        "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
        "                          % (acc(self.y, y_pred), nmi(self.y, y_pred)))\n",
        "\n",
        "            PrintACC(x, y)\n",
        "\n",
        "        # begin pretraining\n",
        "        t0 = time()\n",
        "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs)\n",
        "        print('Pretraining time: %ds' % round(time() - t0))\n",
        "        # self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
        "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
        "        self.pretrained = True\n",
        "\n",
        "    def load_weights(self, weights):  # load weights of DEC model\n",
        "        self.model.load_weights(weights)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        return self.encoder.predict(x)\n",
        "\n",
        "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
        "        q = self.model.predict(x, verbose=0)\n",
        "        return q.argmax(1)\n",
        "\n",
        "    @staticmethod\n",
        "    def target_distribution(q):\n",
        "        weight = q ** 2 / q.sum(0)\n",
        "        return (weight.T / weight.sum(1)).T\n",
        "\n",
        "    def compile(self, optimizer='sgd', loss='kld'):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
        "            update_interval=140, save_dir='./results/temp', save_trail=False, \n",
        "            save_path=None):\n",
        "\n",
        "        print('Update interval', update_interval)\n",
        "        save_interval = max(int(x.shape[0] / batch_size) * 5, 5)  # 5 epochs\n",
        "        print('Save interval', save_interval)\n",
        "\n",
        "        # Step 1: initialize cluster centers using k-means\n",
        "        t1 = time()\n",
        "        print('Initializing cluster centers with k-means.')\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
        "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
        "        y_pred_last = np.copy(y_pred)\n",
        "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
        "\n",
        "        # Step 2: deep clustering\n",
        "        # logging file\n",
        "        import csv\n",
        "        # logfile = open(save_dir + '/dec_log.csv', 'w')\n",
        "        # logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
        "        # logwriter.writeheader()\n",
        "\n",
        "        loss = 0\n",
        "        index = 0\n",
        "        index_array = np.arange(x.shape[0])\n",
        "        for ite in range(int(maxiter)):\n",
        "            if ite % update_interval == 0:\n",
        "                q = self.model.predict(x, verbose=0)\n",
        "                # print(q.shape)\n",
        "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
        "\n",
        "                # evaluate the clustering performance\n",
        "                y_pred = q.argmax(1)\n",
        "                if y is not None:\n",
        "                    acc_ = np.round(acc(y, y_pred), 5)\n",
        "                    nmi_ = np.round(nmi(y, y_pred), 5)\n",
        "                    ari_ = np.round(ari(y, y_pred), 5)\n",
        "                    vm = np.round(sklearn.metrics.v_measure_score(y, y_pred), 5)\n",
        "                    loss = np.round(loss, 5)\n",
        "                    with open(save_path, \"a\") as f:\n",
        "                        f.write(str(acc_) + \"\\n\")\n",
        "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f, vm = %.5f' % (ite, acc_, nmi_, ari_, vm), ' ; loss=', loss)\n",
        "\n",
        "                # check stop criterion\n",
        "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
        "                y_pred_last = np.copy(y_pred)\n",
        "                if ite > 0 and delta_label < tol:\n",
        "                    print('delta_label ', delta_label, '< tol ', tol)\n",
        "                    print('Reached tolerance threshold. Stopping training.')\n",
        "                    # logfile.close()\n",
        "                    break\n",
        "\n",
        "            # train on batch\n",
        "            # if index == 0:\n",
        "            #     np.random.shuffle(index_array)\n",
        "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
        "            loss = self.model.train_on_batch(x=x[idx], y=p[idx])\n",
        "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
        "\n",
        "            # save intermediate model\n",
        "            if ite % save_interval == 0:\n",
        "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
        "                # self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
        "\n",
        "            ite += 1\n",
        "\n",
        "        # save the trained model\n",
        "        # logfile.close()\n",
        "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
        "        # self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "def DEC_main(dataset, files, save=False, save_trail=False, save_path=None):\n",
        "    # # setting the hyper parameters\n",
        "    # import argparse\n",
        "\n",
        "    # parser = argparse.ArgumentParser(description='train',\n",
        "    #                                  formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    # parser.add_argument('--dataset', default='mnist',\n",
        "    #                     choices=['mnist', 'fmnist', 'usps', 'reuters10k', 'stl', 'comps', 'comps_csv'])\n",
        "    # parser.add_argument('--experiment', default='f1_e1',\n",
        "    #                     choices=['f1_e1', 'f1_e2', 'f1_e3', 'f1_e4', 'f1_e5', 'f1_e6', \n",
        "    #                     'f1_e7', 'f1_e8', 'f1_e9', 'f1_e10'])\n",
        "    # parser.add_argument('--batch_size', default=256, type=int)\n",
        "    # parser.add_argument('--maxiter', default=2e4, type=int)\n",
        "    # parser.add_argument('--pretrain_epochs', default=None, type=int)\n",
        "    # parser.add_argument('--update_interval', default=None, type=int)\n",
        "    # parser.add_argument('--tol', default=0.001, type=float)\n",
        "    # parser.add_argument('--ae_weights', default=None)\n",
        "    # parser.add_argument('--save_dir', default='results')\n",
        "    # args = parser.parse_args()\n",
        "    # print(args)\n",
        "\n",
        "    # load dataset\n",
        "    print(\"loading dataset...\")\n",
        "    x, y = load_comps_csv(files)\n",
        "    n_clusters = len(np.unique(y))\n",
        "\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "\n",
        "    init = 'glorot_uniform'\n",
        "    pretrain_optimizer = 'adam'\n",
        "    # setting parameters\n",
        "    if dataset == 'comps_csv':\n",
        "        update_interval = 140\n",
        "        pretrain_epochs = 100\n",
        "        init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
        "                               distribution='uniform')  # [-limit, limit], limit=sqrt(1./fan_in)\n",
        "        pretrain_optimizer = SGD(lr=1, momentum=0.9)\n",
        "    elif dataset == 'comps':\n",
        "        update_interval = 30\n",
        "        pretrain_epochs = 100\n",
        "\n",
        "    # prepare the DEC model\n",
        "    dec = DEC(dims=[x.shape[-1], 500, 500, 2000, 10], n_clusters=n_clusters, init=init)\n",
        "\n",
        "    dec.pretrain(x=x, y=y, optimizer=pretrain_optimizer,\n",
        "                  epochs=pretrain_epochs)\n",
        "\n",
        "    dec.model.summary()\n",
        "    t0 = time()\n",
        "    dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')\n",
        "    y_pred = dec.fit(x, y=y, update_interval=update_interval, save_path=save_path)\n",
        "    print('acc:', acc(y, y_pred))\n",
        "    print('clustering time: ', (time() - t0))\n",
        "\n",
        "    if save:\n",
        "      name = \"_\".join([f[:5] for f in files]) \n",
        "      drive_dir = \"drive/My Drive/10701/\"\n",
        "      with open(drive_dir + \"results/dec_features/f2/{}.txt\".format(name), \"w\") as f:\n",
        "        f.write(\", \".join([f[:-4] for f in files]) + \"\\n\")\n",
        "        f.write(\"acc: {}\\n\".format(acc(y, y_pred)))\n",
        "        f.write(\"vms: {}\\n\".format(vms(y, y_pred)))\n",
        "        f.write(\"nmi: {}\\n\".format(nmi(y, y_pred)))\n",
        "        f.write(\"ari: {}\\n\".format(ari(y, y_pred)))\n",
        "   \n",
        "\n",
        "    return acc(y, y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DvzzcWUoFzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd my-DEC\n",
        "# %load datasets.py\n",
        "# from datasets import load_comps_csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCUgGi5xrM_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x, y = load_comps_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmJoFQomUv0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd my-DEC/\n",
        "# !python DEC.py --dataset comps --experiment f1_e5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlXSV24BFRny",
        "colab_type": "text"
      },
      "source": [
        "# run all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG8kuprLU6l8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "alkan_csv = \"alkan_all.csv\"\n",
        "bach_csv = \"bach_all.csv\"\n",
        "beethoven_csv = \"beethoven_all.csv\"\n",
        "brahms_csv = \"brahms_all.csv\"\n",
        "buxtehude_csv = \"buxtehude_all.csv\"\n",
        "byrd_csv = \"byrd_all.csv\"\n",
        "chopin_csv = \"chopin_all.csv\"\n",
        "dandrieu_csv = \"dandrieu_all.csv\"\n",
        "dvorak_csv = \"dvorak_all.csv\"\n",
        "debussy_csv = \"debussy_all.csv\"\n",
        "faure_csv = \"faure_all.csv\"\n",
        "handel_csv = \"handel_all.csv\"\n",
        "haydn_csv = \"haydn_all.csv\"\n",
        "mozart_csv = \"mozart_all.csv\"\n",
        "scarlatti_csv = \"scarlatti_all.csv\"\n",
        "schubert_csv = \"schubert_all.csv\"\n",
        "schumann_csv = \"schumann_all.csv\"\n",
        "scriabin_csv = \"scriabin_all.csv\"\n",
        "shostakovich_csv = \"shostakovich_all.csv\"\n",
        "soler_csv = \"soler_all.csv\"\n",
        "\n",
        "csv_comps = [alkan_csv, bach_csv, beethoven_csv,\n",
        "               brahms_csv, buxtehude_csv, byrd_csv, \n",
        "               chopin_csv, dandrieu_csv, dvorak_csv, debussy_csv,\n",
        "               faure_csv, handel_csv, haydn_csv,\n",
        "               mozart_csv, scarlatti_csv, schubert_csv,\n",
        "              schumann_csv, scriabin_csv, shostakovich_csv, soler_csv]\n",
        "\n",
        "# data = []\n",
        "# for a in csv_comps:\n",
        "#   tmp = []\n",
        "#   for b in csv_comps:\n",
        "#     name = \"_\".join([f[:5] for f in [a, b]]) \n",
        "#     # if not os.path.exists(\"drive/My Drive/10701/results/dec_features/f1/{}.txt\".format(name)):\n",
        "#     save_path = \"drive/My Drive/10701/results/dec_features/trail/{}.txt\".format(name)\n",
        "#     tmp.append(DEC_main(\"comps_csv\", [a, b], save_trail=True, save_path=save_path))\n",
        "#   data.append(tmp)\n",
        "#   print(data)\n",
        "# data = np.array(data)\n",
        "\n",
        "sys.exit()\n",
        "\n",
        "# print(run_clustering_features(\"kmeans\", [dandrieu_csv, soler_csv], save=True))\n",
        "# np.savetxt(\"drive/My Drive/10701/results/dec_features_accuracy.txt\", data)\n",
        "# data = np.around(np.loadtxt(\"drive/My Drive/10701/results/dec_features_accuracy-2.txt\"), decimals=2)\n",
        "\n",
        "print(data)\n",
        "# sys.exit()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(data, cmap=\"Wistia\")\n",
        "\n",
        "comps_names = [c[:-8] for c in csv_comps]\n",
        "\n",
        "# We want to show all ticks...\n",
        "ax.set_xticks(np.arange(len(comps_names)))\n",
        "ax.set_yticks(np.arange(len(comps_names)))\n",
        "# ... and label them with the respective list entries\n",
        "ax.set_xticklabels(comps_names)\n",
        "ax.set_yticklabels(comps_names)\n",
        "\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "          rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(comps_names)):\n",
        "    for j in range(len(comps_names)):\n",
        "        text = ax.text(j, i, data[i, j],\n",
        "                        ha=\"center\", va=\"center\", color=\"black\", size=\"6\")\n",
        "\n",
        "ax.set_title(\"heatmap of pairwise composer clustering acc - raw piano roll\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7bKSvsYN-im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5f6I8s1HrP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBPr2K-fKF5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}